{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe5b554-b094-4c6f-975f-5142f082b7a8",
   "metadata": {},
   "source": [
    "**installing and importing useful libraries**"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T10:05:46.689432Z",
     "start_time": "2025-10-12T10:05:46.684074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(plt.style.available)"
   ],
   "id": "1adebfbb2aee8411",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn', 'seaborn-bright', 'seaborn-colorblind', 'seaborn-dark', 'seaborn-dark-palette', 'seaborn-darkgrid', 'seaborn-deep', 'seaborn-muted', 'seaborn-notebook', 'seaborn-paper', 'seaborn-pastel', 'seaborn-poster', 'seaborn-talk', 'seaborn-ticks', 'seaborn-white', 'seaborn-whitegrid', 'tableau-colorblind10']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "9a2f0f6f-c322-4fd3-b08e-5d934c84cfad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-12T10:06:51.225863Z",
     "start_time": "2025-10-12T10:06:51.212120Z"
    }
   },
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_theme()\n",
    "\n",
    "#importing optimization techniques\n",
    "from implementations import *"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "aa20b222-c0c6-4fdd-b500-885542b16cb3",
   "metadata": {},
   "source": [
    "**importing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9a5b36-c5a1-426f-a6eb-15117ba48dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data from dataprocessing section...\n",
    "# so we have x_train and y_train dataprocessed IMPORTANT y _train must be from {-1, 1} values to {0, 1} values and dataprocessed as well (clean outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7578a-8712-481d-a6d8-9a12ba3e71dd",
   "metadata": {},
   "source": [
    "parameter tuning for regularized logistic regression with GD and Adam using k-fold cross validation"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def make_balanced_subset(x_train_filtered, y_train, majority_class=-1, minority_class=1,\n",
    "                         seed_major=0, seed_minor=42, seed_shuffle=7):\n",
    "    # Boolean masks\n",
    "    maj_mask = (y_train == majority_class)\n",
    "    min_mask = (y_train == minority_class)\n",
    "\n",
    "    # Indices per class\n",
    "    maj_idx = np.nonzero(maj_mask)[0]\n",
    "    min_idx = np.nonzero(min_mask)[0]\n",
    "\n",
    "    # Target size = size of minority (undersample majority)\n",
    "    n = len(min_idx)\n",
    "\n",
    "    # Sample without replacement\n",
    "    rs_maj = np.random.RandomState(seed_major)\n",
    "    rs_min = np.random.RandomState(seed_minor)\n",
    "    sampled_maj = rs_maj.choice(maj_idx, size=n, replace=False)\n",
    "    sampled_min = rs_min.choice(min_idx, size=n, replace=False)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_idx = np.concatenate([sampled_maj, sampled_min])\n",
    "    rs_shuf = np.random.RandomState(seed_shuffle)\n",
    "    rs_shuf.shuffle(balanced_idx)\n",
    "\n",
    "    # Slice arrays\n",
    "    x_bal = x_train_filtered[balanced_idx]\n",
    "    y_bal = y_train[balanced_idx]\n",
    "    return x_bal, y_bal, balanced_idx\n",
    "\n",
    "x_train_balanced, y_train_balanced, balanced_indices = make_balanced_subset(\n",
    "    x_train_filtered, y_train,\n",
    "    majority_class=-1, minority_class=1,\n",
    "    seed_major=0, seed_minor=42, seed_shuffle=7\n",
    ")\n",
    "\n",
    "# x_train_balanced.shape, y_train_balanced.shape\n"
   ],
   "id": "6e037f021edc1a98"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9431e08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### hyperparamter tuning regularized logistic regression with GD\n",
    "\n",
    "param_grid = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])  # lambdas_\n",
    "N = len(y_train)\n",
    "d = x_train.shape[1]\n",
    "k = 10  # folds\n",
    "dim = int(np.ceil(N / k))  # length of the test set\n",
    "initial_w = np.zeros(d, )\n",
    "max_iters = 10000\n",
    "gamma = 0.01\n",
    "\n",
    "#### validation metrics\n",
    "logistic_loss = np.zeros(len(param_grid),)\n",
    "MSE = np.zeros(len(param_grid),)\n",
    "AUC=  np.zeros(len(param_grid),)\n",
    "\n",
    "####  k-fold cross validation\n",
    "for p, lam in enumerate(param_grid):\n",
    "    fold_loss = 0\n",
    "    fold_MSE = 0\n",
    "    fold_AUC = 0\n",
    "\n",
    "    for i in range(k):\n",
    "    # test fold\n",
    "        x_test_i = x_train[i*dim : min((i+1)*dim, N), :]\n",
    "        y_test_i = y_train[i*dim : min((i+1)*dim, N), :]\n",
    "    \n",
    "    # train folds\n",
    "        x_train_i = np.concatenate([x_train[:i*dim, :], x_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "        y_train_i = np.concatenate([y_train[:i*dim, :], y_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "\n",
    "    #train the model with parameter p\n",
    "        w, loss = reg_logistic_regression(y_train_i, x_train_i, lam ,initial_w,max_iters, gamma) \n",
    "\n",
    "    # storing validation results\n",
    "        fold_loss += loss\n",
    "        fold_MSE += compute_mse_loss(y_train_i, x_train_i, w) #train error SEE IF IT IS BETTER TO USE np.mean((y_train_i - y_pred_train)**2)\n",
    "        predictions = sigmoid (x_test_i @ w)\n",
    "        fold_AUC += compute_auc(y_test_i, predictions)\n",
    "\n",
    "\n",
    "#### mean of results  \n",
    "    logistic_loss[p] = fold_loss / k\n",
    "    MSE[p] = fold_MSE / k\n",
    "    AUC[p] = fold_AUC / k\n",
    "\n",
    "\n",
    "#### Plotting results per lambda\n",
    "\n",
    "num_par = len(param_grid)\n",
    "w = 0.3 # bar width\n",
    "pos = np.arange(num_par)\n",
    "plt.bar(pos - w, AUC, width = w, label='AUC' )\n",
    "plt.bar(pos, MSE, width=w, label= 'MSE')\n",
    "plt.bar(pos + w, logistic_loss, width = w, label = 'logistic loss' )\n",
    "\n",
    "plt.xticks(pos, param_grid)\n",
    "plt.xlabel('Different regularization hyperparameter values')\n",
    "plt.title('Finding the best regularization hyperparamter - GD case')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC calcolata manualmente: 0.64\n",
      "AUC con scikit-learn: 0.64\n",
      "Differenza: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#### hyperparamter tuning regularized logistic regression with Adam\n",
    "\n",
    "param_grid = np.array([0.001, 0.01, 0.1, 1, 10, 100, 1000])  # lambdas_\n",
    "N = len(y_train)\n",
    "d = x_train.shape[1]\n",
    "k = 10  # folds\n",
    "dim = int(np.ceil(N / k))  # length of the test set\n",
    "initial_w = np.zeros(d, )\n",
    "max_iters = 10000\n",
    "gamma = 0.01\n",
    "beta_1 = 0.9 #using Adam paper as benchmark\n",
    "beta_2 = 0.999 #using Adam paper as benchmark\n",
    "mini_batch_size = 700 #using Adam paper as benchmark \n",
    "\n",
    "#### validation metrics\n",
    "logistic_loss = np.zeros(len(param_grid),)\n",
    "MSE = np.zeros(len(param_grid),)\n",
    "AUC=  np.zeros(len(param_grid),)\n",
    "\n",
    "####  k-fold cross validation\n",
    "for p, lam in enumerate(param_grid):\n",
    "    fold_loss = 0\n",
    "    fold_MSE = 0\n",
    "    fold_AUC = 0\n",
    "\n",
    "    for i in range(k):\n",
    "    # test fold\n",
    "        x_test_i = x_train[i*dim : min((i+1)*dim, N), :]\n",
    "        y_test_i = y_train[i*dim : min((i+1)*dim, N), :]\n",
    "    \n",
    "    # train folds\n",
    "        x_train_i = np.concatenate([x_train[:i*dim, :], x_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "        y_train_i = np.concatenate([y_train[:i*dim, :], y_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "\n",
    "    #train the model with parameter p\n",
    "        w, loss = reg_logistic_regression_adam(y_train_i, x_train_i, lam ,initial_w,max_iters, beta_1, beta_2, gamma, mini_batch_size) \n",
    "\n",
    "    # storing validation results\n",
    "        fold_loss += loss\n",
    "        fold_MSE += compute_mse_loss(y_train_i, x_train_i, w) #train error SEE IF IT IS BETTER TO USE np.mean((y_train_i - y_pred_train)**2)\n",
    "        predictions = sigmoid (x_test_i @ w)\n",
    "        fold_AUC += compute_auc(y_test_i, predictions)\n",
    "\n",
    "\n",
    "#### mean of results  \n",
    "    logistic_loss[p] = fold_loss / k\n",
    "    MSE[p] = fold_MSE / k\n",
    "    AUC[p] = fold_AUC / k\n",
    "\n",
    "\n",
    "#### Plotting results per lambda\n",
    "\n",
    "num_par = len(param_grid)\n",
    "w = 0.3 # bar width\n",
    "pos = np.arange(num_par)\n",
    "plt.bar(pos - w, AUC, width = w, label='AUC' )\n",
    "plt.bar(pos, MSE, width=w, label= 'MSE')\n",
    "plt.bar(pos + w, logistic_loss, width = w, label = 'logistic loss' )\n",
    "\n",
    "plt.xticks(pos, param_grid)\n",
    "plt.xlabel('Different regularization hyperparameter values')\n",
    "plt.title('Finding the best regularization hyperparamter - Adam case')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "    \n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5d4d1",
   "metadata": {},
   "source": [
    "Now that we have found the best regularization hyperparameter for Adam reg log and GD reg log, let's test which model is the best to make predictions using a k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFRONTING ALL THE MODELS WITH K-FOLD CROSS VALIDATION \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = len(y_train)\n",
    "d = x_train.shape[1]\n",
    "k = 10  # folds\n",
    "dim = int(np.ceil(N / k))  # length of the test set\n",
    "initial_w = np.zeros(d, )\n",
    "max_iters = 10000\n",
    "gamma = 0.01\n",
    "beta_1 = 0.9 #using Adam paper as benchmark\n",
    "beta_2 = 0.999 #using Adam paper as benchmark\n",
    "mini_batch_size = 700 #using Adam paper as benchmark \n",
    "\n",
    "models = [\n",
    "    (\"MSE GD\", lambda: mean_squared_error_gd(initial_w=initial_w, max_iters=max_iters, gamma=gamma)),\n",
    "    (\"MSE SGD\", lambda: mean_squared_error_sgd(initial_w=initial_w, max_iters=max_iters, gamma=gamma, mini_batch_size=mini_batch_size)),\n",
    "    (\"Least Squares\", lambda: least_squares()),\n",
    "    (\"Ridge Regression\", lambda: ridge_regression()), #understand which lambda here \n",
    "    (\"Logistic Regression GD\", lambda: logistic_regression(initial_w=initial_w, max_iters=max_iters, gamma=gamma)),\n",
    "    (\"Reg Logistic ADAM\", lambda: reg_logistic_regression_adam(lambda_adam, initial_w=initial_w, max_iters=max_iters,  beta_1=beta_1, beta_2=beta_2,gamma=gamma, mini_batch_size=mini_batch_size)),\n",
    "    (\"Reg Logistic GD\", lambda: reg_logistic_regression(lambda_gd, initial_w=initial_w, max_iters=max_iters, gamma=gamma))\n",
    "]  ### does it make sense to include all the models ????\n",
    "\n",
    "\n",
    "#### validation metrics\n",
    "logistic_loss = np.zeros(len(models),)\n",
    "MSE = np.zeros(len(models),)\n",
    "AUC=  np.zeros(len(models),)\n",
    "\n",
    "####  k-fold cross validation\n",
    "for p, model in enumerate(models):\n",
    "    fold_loss = 0\n",
    "    fold_MSE = 0\n",
    "    fold_AUC = 0\n",
    "\n",
    "    for i in range(k):\n",
    "    # test fold\n",
    "        x_test_i = x_train[i*dim : min((i+1)*dim, N), :]\n",
    "        y_test_i = y_train[i*dim : min((i+1)*dim, N), :]\n",
    "    \n",
    "    # train folds\n",
    "        x_train_i = np.concatenate([x_train[:i*dim, :], x_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "        y_train_i = np.concatenate([y_train[:i*dim, :], y_train[min((i+1)*dim, N):, :]], axis=0)\n",
    "\n",
    "    #train the model \n",
    "        name, model_fn = models[p] \n",
    "        w, loss = model_fn(y_train_i, x_train_i)   \n",
    "\n",
    "    # storing validation results\n",
    "        fold_loss += loss\n",
    "        fold_MSE += compute_mse_loss(y_train_i, x_train_i, w) #train error SEE IF IT IS BETTER TO USE np.mean((y_train_i - y_pred_train)**2)\n",
    "        predictions = sigmoid (x_test_i @ w)\n",
    "        fold_AUC += compute_auc(y_test_i, predictions)\n",
    "\n",
    "\n",
    "#### mean of results  \n",
    "    logistic_loss[p] = fold_loss / k\n",
    "    MSE[p] = fold_MSE / k\n",
    "    AUC[p] = fold_AUC / k\n",
    "\n",
    "\n",
    "#### Plotting results per model   FIX HERE \n",
    "\n",
    "num_models = len(models)\n",
    "w = 0.3\n",
    "pos = np.arange(num_models)\n",
    "labels = [name for name, _ in models]\n",
    "\n",
    "plt.bar(pos - w, AUC, width=w, label='AUC')\n",
    "plt.bar(pos, MSE, width=w, label='MSE')\n",
    "plt.bar(pos + w, logistic_loss, width=w, label='logistic loss')\n",
    "\n",
    "plt.xticks(pos, labels, rotation=45)\n",
    "plt.xlabel('Different models')\n",
    "plt.title('Finding the best model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
