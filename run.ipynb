{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe5b554-b094-4c6f-975f-5142f082b7a8",
   "metadata": {},
   "source": [
    "**installing and importing useful libraries**"
   ]
  },
  {
   "cell_type": "code",
   "id": "9a2f0f6f-c322-4fd3-b08e-5d934c84cfad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:05:43.069148Z",
     "start_time": "2025-10-13T19:05:43.064996Z"
    }
   },
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_theme()\n",
    "\n",
    "#importing optimization techniques\n",
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from helpers import *"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "aa20b222-c0c6-4fdd-b500-885542b16cb3",
   "metadata": {},
   "source": [
    "**importing data**"
   ]
  },
  {
   "cell_type": "code",
   "id": "5a9a5b36-c5a1-426f-a6eb-15117ba48dba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:51:52.057246Z",
     "start_time": "2025-10-13T19:51:50.872986Z"
    }
   },
   "source": [
    "#IF WE USE OUTLIERS DETECTION WE SHOULD ACHANGE Y TRAIN\n",
    "\n",
    "\n",
    "# Paths to the CSV files ---- > change with yours\n",
    "datapath_1 = \"/Users/neilabenlamri/PycharmProjects/project-1-girl_power/data/dataset/pca_train_projection.csv\"\n",
    "datapath_2 = \"/Users/neilabenlamri/PycharmProjects/project-1-girl_power/data/dataset/y_train.csv\"\n",
    "\n",
    "\n",
    "# Load the data\n",
    "x_train = np.loadtxt(datapath_1, delimiter=',', dtype=float, skiprows=1)\n",
    "y_train = np.loadtxt(datapath_2, delimiter=',' , dtype=float, skiprows=1, usecols=1)\n",
    "\n",
    "# Convert y_train from {-1, 1} to {0, 1}\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (328135, 60)\n",
      "y_train shape: (328135,)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "e88279ec",
   "metadata": {},
   "source": [
    "### Splitting the data between 20 % validation set and 80 % training set"
   ]
  },
  {
   "cell_type": "code",
   "id": "7488fbfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:51:57.744092Z",
     "start_time": "2025-10-13T19:51:57.629589Z"
    }
   },
   "source": [
    "def train_val_split(X, y, val_ratio=0.20, seed=42):\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_val = int(np.round(n * val_ratio))\n",
    "    val_idx = idx[:n_val]\n",
    "    train_idx = idx[n_val:]\n",
    "\n",
    "    return X[train_idx], X[val_idx], y[train_idx], y[val_idx], train_idx, val_idx\n",
    "\n",
    "\n",
    "\n",
    "x_train_split, x_val_split, y_train_split, y_val_split, tr_idx, va_idx = train_val_split(x_train, y_train, val_ratio=0.20, seed=42)\n",
    "print(x_train_split.shape)\n",
    "print(x_val_split.shape)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262508, 60)\n",
      "(65627, 60)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "9a26b47e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:52:01.058341Z",
     "start_time": "2025-10-13T19:52:01.051135Z"
    }
   },
   "source": [
    "# OVERSAMPLING AND BALANCING DATA\n",
    "def make_balanced_subset(x_train_filtered, y_train, majority_class=-1, minority_class=1,\n",
    "                         seed_major=0, seed_minor=42, seed_shuffle=7):\n",
    "    # Boolean masks\n",
    "    maj_mask = (y_train == majority_class)\n",
    "    min_mask = (y_train == minority_class)\n",
    "\n",
    "    # Indices per class\n",
    "    maj_idx = np.nonzero(maj_mask)[0]\n",
    "    min_idx = np.nonzero(min_mask)[0]\n",
    "\n",
    "    # Target size = size of minority (undersample majority)\n",
    "    n = len(min_idx)\n",
    "\n",
    "    # Sample without replacement\n",
    "    rs_maj = np.random.RandomState(seed_major)\n",
    "    rs_min = np.random.RandomState(seed_minor)\n",
    "    sampled_maj = rs_maj.choice(maj_idx, size=n, replace=False)\n",
    "    sampled_min = rs_min.choice(min_idx, size=n, replace=False)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_idx = np.concatenate([sampled_maj, sampled_min])\n",
    "    rs_shuf = np.random.RandomState(seed_shuffle)\n",
    "    rs_shuf.shuffle(balanced_idx)\n",
    "\n",
    "    # Slice arrays\n",
    "    x_bal = x_train_filtered[balanced_idx]\n",
    "    y_bal = y_train[balanced_idx]\n",
    "    return x_bal, y_bal, balanced_idx\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATA --- > not needed with pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7578a-8712-481d-a6d8-9a12ba3e71dd",
   "metadata": {},
   "source": [
    "### Hyperparameters definition and metrics"
   ]
  },
  {
   "cell_type": "code",
   "id": "72cf219e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:52:07.013493Z",
     "start_time": "2025-10-13T19:52:07.004648Z"
    }
   },
   "source": [
    "#lambdas = [ 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1]  # regularization parameters list\n",
    "#gammas = [1e-4, 1e-3, 1e-2, 1e-1, 1] # step-size parameters list\n",
    "#max_iters = [100, 1000, 10000] # max iters list\n",
    "\n",
    "lambdas = [ 1e-4, 1e-3, 1e-2, 1e-1]  # regularization parameters list\n",
    "gammas = [1e-4, 1e-3] # step-size parameters list\n",
    "max_iters = [100] # max iters list\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    AUC calculation using Mann-Whitney statistics\n",
    "    Inputs : \n",
    "            - y_true : numpy array containing the real {0, 1} values of the dataset\n",
    "            - y_scores : numpy array containing our predictions\n",
    "    Output : \n",
    "            AUC Area under the ROC curve \n",
    "    \"\"\"\n",
    "    order = np.argsort(y_scores)\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    n_pos = np.sum(y_true)\n",
    "    n_neg = len(y_true) - n_pos\n",
    "\n",
    "    # rank positions \n",
    "    rank_positions = np.arange(1, len(y_true_sorted) + 1)\n",
    "    rank_sum = np.sum(rank_positions[y_true_sorted == 1])\n",
    "\n",
    "    # AUC using Mann–Whitney\n",
    "    auc = (rank_sum - n_pos*(n_pos+1)/2) / (n_pos * n_neg)\n",
    "    return auc\n",
    "\n",
    "def compute_accuracy(y_true, y_scores) : \n",
    "    \"\"\"\n",
    "    Accuracy computation\n",
    "    Inputs : \n",
    "            - y_true : numpy array containing the real {0, 1} values of the dataset\n",
    "            - y_scores : numpy array containing our predictions\n",
    "    Output : \n",
    "            Accuracy = correct predictions / total predictions %\n",
    "\n",
    "    \"\"\"\n",
    "    correct_pred = (y_true == y_scores)\n",
    "    accuracy = np.mean(correct_pred) * 100\n",
    "    return accuracy\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "6fa264a6",
   "metadata": {},
   "source": [
    "### K-fold cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "id": "9431e08b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:54:53.988587Z",
     "start_time": "2025-10-13T19:52:10.867897Z"
    }
   },
   "source": [
    "\n",
    "def build_k_indices(N, k_fold, seed=21): \n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        N:      num of samples\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    \"\"\"\n",
    "    num_row = N\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "     \n",
    "\n",
    "def k_fold_cross_validation(y_train, x_train, lambdas, gammas, max_iters, k_fold, methods, seed, oversampling = True) :\n",
    "    # dictionary to contain the best method with the best parameters and its metrics\n",
    "    best_overall = {\"method\": \"\", \"lambda_\": 0, \"gamma\": 0, \"max_iter\": 0, \"train loss\": 0, \"test loss\": 0, \"AUC\": 0, \"accuracy\": 0, \"y_pred\" : None, \"w_opt\": None}\n",
    "    results =[] # to keep the best results per method\n",
    "    #creating k folders on the train set \n",
    "    k_indices = build_k_indices(len(y_train), k_fold, seed)\n",
    "\n",
    "    for method in methods : #scrolling methods\n",
    "        best_per_method =  {\"method\": \"\", \"lambda_\": 0, \"gamma\": 0, \"max_iter\": 0, \"train loss\": 0, \"test loss\": 0, \"AUC\": 0, \"accuracy\": 0, \"y_pred\" : None, \"w_opt\": None} \n",
    "        for lam in lambdas : # scrolling lambdas\n",
    "            for gam in gammas : #scrolling gammas\n",
    "                for max_it in max_iters : #scrolling iters ----> model defined at this point\n",
    "                    logistic_loss_tr = []\n",
    "                    logistic_loss_te =[]\n",
    "                    AUC=  []\n",
    "                    accuracies = []\n",
    "                    \n",
    "                    for k in range(k_fold) : \n",
    "                        #k-th subgroup in test, others in train\n",
    "                        test_mask = np.isin(np.arange(len(y_train)), k_indices[k, :])\n",
    "                        y_test_k = y_train[test_mask]\n",
    "                        x_test_k = x_train[test_mask]\n",
    "    \n",
    "                        y_train_k=y_train[~test_mask]\n",
    "                        x_train_k=x_train[~test_mask]\n",
    "                        \n",
    "                        if oversampling :\n",
    "                            #oversampling dataset\n",
    "                            x_train_k , y_train_k, _ = make_balanced_subset(x_train_k, y_train_k, 0, 1, seed_major=0, seed_minor=42, seed_shuffle=7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        #train the model\n",
    "                        if method == \"reg_logistic_regression\" :\n",
    "                            w_opt, loss = reg_logistic_regression(y_train_k, x_train_k,lam, np.zeros(x_train_k.shape[1]), max_it, gam)\n",
    "                        elif method == \"least_squares\" :\n",
    "                            w_opt, loss = least_squares(y_train_k, x_train_k)\n",
    "                        elif method == \"adam_reg_logistic_regression\":\n",
    "                            w_opt, loss = reg_logistic_regression_adam(y_train_k, x_train_k, lam, np.zeros(x_train_k.shape[1]), max_it, 0.9, 0.999, gam, 700 )\n",
    "                        elif method == \"ridge_regression\" :\n",
    "                            w_opt, loss = ridge_regression(y_train_k, x_train_k, lam)\n",
    "\n",
    "                        #computing metrics \n",
    "                        logistic_loss_tr.append(compute_logistic_loss(y_train_k, x_train_k, w_opt)) #without penalizing term\n",
    "                        logistic_loss_te.append(compute_logistic_loss(y_test_k, x_test_k, w_opt)) #without penalizing term CAPIRE CHE SENSO HA COMPARARE STE LOSS\n",
    "\n",
    "                        if method in [\"adam_reg_logistic_regression\", \"reg_logistic_regression\"]:\n",
    "                            pred = sigmoid(x_test_k @ w_opt) \n",
    "                        else:\n",
    "                            pred = x_test_k @ w_opt\n",
    "\n",
    "                        AUC.append(compute_auc(y_test_k, pred))\n",
    "                        accuracies.append(compute_accuracy(y_test_k, (pred >= 0.5).astype(int)))\n",
    "                    # updating    \n",
    "                    if np.mean(AUC) > best_per_method[\"AUC\"]:\n",
    "                        best_per_method.update({\"method\": method, \"gamma\": gam, \"lambda_\": lam, \"max_iter\": max_it, \"train loss\": np.mean(logistic_loss_tr), \"test loss\": np.mean(logistic_loss_te), \"AUC\": np.mean(AUC), \"accuracy\": np.mean(accuracies), \"y_pred\" : (pred >= 0.5).astype(int) , \"w_opt\": w_opt})\n",
    "\n",
    "                    if np.mean(AUC) > best_overall[\"AUC\"]:\n",
    "                        best_overall.update({\"method\": method, \"gamma\": gam, \"lambda_\": lam, \"max_iter\": max_it, \"train loss\": np.mean(logistic_loss_tr), \"test loss\": np.mean(logistic_loss_te), \"AUC\": np.mean(AUC), \"accuracy\": np.mean(accuracies), \"y_pred\" : (pred >= 0.5).astype(int) , \"w_opt\": w_opt})\n",
    "\n",
    "        results.append(best_per_method)\n",
    "        print(f\"For method {method}, best λ={best_per_method['lambda_']}, γ={best_per_method['gamma']}, max_iter={best_per_method['max_iter']}\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"The best method is {best_overall['method']}, with best λ={best_overall['lambda_']}, γ={best_overall['gamma']}, max_iter={best_overall['max_iter']}\")\n",
    "\n",
    "\n",
    "\n",
    "    return best_overall, results \n",
    "\n",
    "        \n",
    "methods=[\"reg_logistic_regression\", \"least_squares\", \"adam_reg_logistic_regression\", \"ridge_regression\"]\n",
    "best_method, results = k_fold_cross_validation(y_train_split, x_train_split, lambdas, gammas, max_iters, 5, methods, seed = 21, oversampling=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For method reg_logistic_regression, best λ=0.0001, γ=0.001, max_iter=100\n",
      "For method least_squares, best λ=0.0001, γ=0.0001, max_iter=100\n",
      "For method adam_reg_logistic_regression, best λ=0.1, γ=0.001, max_iter=100\n",
      "For method ridge_regression, best λ=0.1, γ=0.0001, max_iter=100\n",
      "The best method is ridge_regression, with best λ=0.1, γ=0.0001, max_iter=100\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "4e3ac5c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T19:55:29.857467Z",
     "start_time": "2025-10-13T19:55:29.848106Z"
    }
   },
   "source": [
    "print(best_method) \n",
    "print(results)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'ridge_regression', 'lambda_': 0.1, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6822609623977103, 'test loss': 0.6822688720494428, 'AUC': 0.8495749109319746, 'accuracy': 91.17197767661568, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.41701858e-02, -8.57237163e-03,  1.56383468e-03, -5.50011864e-04,\n",
      "        4.29371982e-03,  1.08214384e-02, -9.40474552e-03, -1.09135964e-02,\n",
      "       -1.87978022e-02, -3.49819476e-04,  8.92197078e-03, -5.72526585e-03,\n",
      "        2.38404292e-03,  1.74682936e-02, -5.14353827e-03, -1.46119664e-02,\n",
      "       -5.69444551e-03,  6.45979725e-03,  6.39013233e-03,  6.24638904e-03,\n",
      "        1.32962532e-02, -8.34851992e-04, -4.50626188e-04, -4.90655481e-03,\n",
      "        3.90526624e-03, -7.52640965e-03,  5.38449923e-03, -8.85165204e-03,\n",
      "       -5.06578449e-03,  3.56742992e-03, -1.88480608e-03, -6.85890782e-04,\n",
      "        5.66216435e-03,  1.86427124e-03, -1.12577010e-03, -4.83110887e-03,\n",
      "        4.23838711e-03,  1.13252708e-03, -1.02152826e-03,  8.66687113e-03,\n",
      "        5.05526419e-03, -1.95168710e-04,  8.34660156e-05,  2.84198810e-03,\n",
      "        1.41601549e-03, -1.05411772e-03,  2.04642832e-04, -9.10249141e-04,\n",
      "        1.41625790e-03,  9.11775643e-04,  2.63632513e-03,  1.12862177e-03,\n",
      "       -1.77697817e-03, -1.08323173e-04, -2.35214750e-04, -4.23377024e-03,\n",
      "       -2.32019380e-03, -2.72145340e-03, -4.04157168e-03, -1.14423319e-03])}\n",
      "[{'method': 'reg_logistic_regression', 'lambda_': 0.0001, 'gamma': 0.001, 'max_iter': 100, 'train loss': 0.6868764418273539, 'test loss': 0.6868852975468813, 'AUC': 0.8343951963000361, 'accuracy': 61.66815870173902, 'y_pred': array([0, 1, 1, ..., 0, 0, 0]), 'w_opt': array([ 2.39176791e-02, -4.18510712e-03,  4.64537601e-04, -1.86198803e-04,\n",
      "        1.19102402e-03,  2.12855812e-03, -1.69820645e-03, -1.86734077e-03,\n",
      "       -3.03861541e-03, -4.58990592e-05,  1.39984307e-03, -9.50850183e-04,\n",
      "        4.08719594e-04,  2.23679671e-03, -5.74858732e-04, -1.84581964e-03,\n",
      "       -7.57491968e-04,  8.98689096e-04,  7.33681419e-04,  6.82368385e-04,\n",
      "        1.57912399e-03,  4.87251314e-05, -8.99664312e-05, -4.74745176e-04,\n",
      "        4.55995950e-04, -8.22153171e-04,  6.14966521e-04, -9.74637419e-04,\n",
      "       -5.82138664e-04,  3.68051482e-04, -2.06571775e-04, -6.08454347e-06,\n",
      "        4.67779161e-04,  1.54233799e-04, -2.46036301e-05, -4.44932536e-04,\n",
      "        4.82357000e-04,  1.07745799e-04, -1.81061364e-04,  7.76454933e-04,\n",
      "        3.99875576e-04,  2.77720971e-05,  6.81512065e-05,  3.16044042e-04,\n",
      "        1.20335115e-04, -1.19293123e-04,  2.33887717e-05, -5.84245976e-05,\n",
      "        2.02195924e-04,  3.42810105e-05,  2.21595787e-04,  1.66021235e-04,\n",
      "       -1.52026415e-04, -4.82611150e-06,  2.15257462e-05, -3.85422032e-04,\n",
      "       -1.65501998e-04, -1.78700388e-04, -2.79104780e-04, -1.24887253e-04])}, {'method': 'least_squares', 'lambda_': 0.0001, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6816663068069607, 'test loss': 0.6816764041256543, 'AUC': 0.8488572131059865, 'accuracy': 91.21654825622369, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.51013108e-02, -8.91045099e-03,  1.66287619e-03, -5.96242010e-04,\n",
      "        4.68221158e-03,  1.19616530e-02, -1.04608280e-02, -1.22907316e-02,\n",
      "       -2.14178750e-02, -4.00719735e-04,  1.02560556e-02, -6.59671068e-03,\n",
      "        2.77323092e-03,  2.05460899e-02, -6.07286979e-03, -1.73069126e-02,\n",
      "       -6.79517362e-03,  7.72016919e-03,  7.65134931e-03,  7.49719025e-03,\n",
      "        1.60669849e-02, -9.99682473e-04, -5.37790322e-04, -5.97064696e-03,\n",
      "        4.75709824e-03, -9.21071031e-03,  6.57420809e-03, -1.08583286e-02,\n",
      "       -6.22567688e-03,  4.38273252e-03, -2.32268638e-03, -8.54338276e-04,\n",
      "        7.03347460e-03,  2.36174752e-03, -1.41912387e-03, -6.08663644e-03,\n",
      "        5.30588144e-03,  1.43109757e-03, -1.30313061e-03,  1.11316413e-02,\n",
      "        6.49138945e-03, -2.55929690e-04,  9.75918564e-05,  3.70688147e-03,\n",
      "        1.86295839e-03, -1.35300257e-03,  2.67050424e-04, -1.19017218e-03,\n",
      "        1.89169814e-03,  1.21711690e-03,  3.47813045e-03,  1.53363102e-03,\n",
      "       -2.40057573e-03, -1.40105606e-04, -3.00626865e-04, -5.80349537e-03,\n",
      "       -3.20450056e-03, -3.78637093e-03, -5.60142140e-03, -1.55824967e-03])}, {'method': 'adam_reg_logistic_regression', 'lambda_': 0.1, 'gamma': 0.001, 'max_iter': 100, 'train loss': 0.6715155162772339, 'test loss': 0.6716819055681588, 'AUC': 0.8446353359491316, 'accuracy': 65.48941924915717, 'y_pred': array([0, 1, 0, ..., 0, 0, 0]), 'w_opt': array([ 0.08332839, -0.0319257 ,  0.00540003, -0.0013986 ,  0.01525461,\n",
      "        0.03080601, -0.01947571, -0.031815  , -0.05340619,  0.00148525,\n",
      "        0.02575117, -0.01819679,  0.00354272,  0.03715674, -0.00412379,\n",
      "       -0.0357545 , -0.02335742,  0.01964969,  0.01824047,  0.01438706,\n",
      "        0.03524786, -0.003799  , -0.00648982, -0.0165444 ,  0.01152711,\n",
      "       -0.01999387,  0.02215926, -0.02042009, -0.02211002,  0.00625984,\n",
      "       -0.00705865, -0.00600952,  0.00889653,  0.00312952, -0.0098822 ,\n",
      "       -0.00976923,  0.01385981,  0.00248412,  0.00533852,  0.02083295,\n",
      "        0.01450082, -0.00196523,  0.00090343,  0.0072388 ,  0.01008791,\n",
      "       -0.00522368, -0.00266878,  0.00042967,  0.01168154, -0.00087542,\n",
      "        0.00128729,  0.00545295, -0.00142774, -0.00714965, -0.00257268,\n",
      "       -0.01238656, -0.00418788,  0.00352045, -0.00778784, -0.01919175])}, {'method': 'ridge_regression', 'lambda_': 0.1, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6822609623977103, 'test loss': 0.6822688720494428, 'AUC': 0.8495749109319746, 'accuracy': 91.17197767661568, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.41701858e-02, -8.57237163e-03,  1.56383468e-03, -5.50011864e-04,\n",
      "        4.29371982e-03,  1.08214384e-02, -9.40474552e-03, -1.09135964e-02,\n",
      "       -1.87978022e-02, -3.49819476e-04,  8.92197078e-03, -5.72526585e-03,\n",
      "        2.38404292e-03,  1.74682936e-02, -5.14353827e-03, -1.46119664e-02,\n",
      "       -5.69444551e-03,  6.45979725e-03,  6.39013233e-03,  6.24638904e-03,\n",
      "        1.32962532e-02, -8.34851992e-04, -4.50626188e-04, -4.90655481e-03,\n",
      "        3.90526624e-03, -7.52640965e-03,  5.38449923e-03, -8.85165204e-03,\n",
      "       -5.06578449e-03,  3.56742992e-03, -1.88480608e-03, -6.85890782e-04,\n",
      "        5.66216435e-03,  1.86427124e-03, -1.12577010e-03, -4.83110887e-03,\n",
      "        4.23838711e-03,  1.13252708e-03, -1.02152826e-03,  8.66687113e-03,\n",
      "        5.05526419e-03, -1.95168710e-04,  8.34660156e-05,  2.84198810e-03,\n",
      "        1.41601549e-03, -1.05411772e-03,  2.04642832e-04, -9.10249141e-04,\n",
      "        1.41625790e-03,  9.11775643e-04,  2.63632513e-03,  1.12862177e-03,\n",
      "       -1.77697817e-03, -1.08323173e-04, -2.35214750e-04, -4.23377024e-03,\n",
      "       -2.32019380e-03, -2.72145340e-03, -4.04157168e-03, -1.14423319e-03])}]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "b59e6c59",
   "metadata": {},
   "source": [
    "### Validation on our validation sample"
   ]
  },
  {
   "cell_type": "code",
   "id": "81de6620",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T20:02:32.431562Z",
     "start_time": "2025-10-13T20:02:32.362389Z"
    }
   },
   "source": [
    "def validation(w_opt, x_val, y_val): \n",
    "    w_opt = best_method[\"w_opt\"]\n",
    "    if best_method[\"method\"] in [\"reg_logistic_regression\", \"adam_reg_logistic_regression\"] :\n",
    "        predictions = sigmoid(x_val @ w_opt) \n",
    "    else:\n",
    "        predictions = x_val @ w_opt\n",
    "    AUC = compute_auc(y_val, predictions)\n",
    "    accuracy = compute_accuracy(y_val, (predictions>=0.5).astype(int))\n",
    "\n",
    "    return AUC, accuracy\n",
    "\n",
    "\n",
    "AUC, accuracy = validation(best_method[\"w_opt\"], x_val_split, y_val_split) \n",
    "\n",
    "print(f\"the best method {best_method['method']} has an accuracy = {accuracy} and an AUC = {AUC} on our validation set\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best method ridge_regression has an accuracy = 91.27950386273942 and an AUC = 0.847318401354341 on our validation set\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "ab264da0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T20:02:36.942804Z",
     "start_time": "2025-10-13T20:02:36.366130Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "cross_validation_visualization(param_grid, logistic_loss_tr, logistic_loss_te)\n",
    "\n",
    "num_par = len(param_grid)\n",
    "w = 0.3 # bar width\n",
    "pos = np.arange(num_par)\n",
    "plt.bar(pos - w, AUC, width = w, label='AUC' )\n",
    "plt.bar(pos, accuracies, width=w, label= 'Accuracy')\n",
    "plt.bar(pos + w, logistic_loss_tr, width = w, label = 'train logistic loss' )\n",
    "plt.bar(pos + w, logistic_loss_te, width=w, label= \"test logistic loss\" )\n",
    "\n",
    "plt.xticks(pos, param_grid)\n",
    "plt.xlabel('Different regularization hyperparameter values')\n",
    "plt.title('Finding the best regularization hyperparamter - ADAM case')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'param_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m cross_validation_visualization(\u001B[43mparam_grid\u001B[49m, logistic_loss_tr, logistic_loss_te)\n\u001B[1;32m      3\u001B[0m num_par \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(param_grid)\n\u001B[1;32m      4\u001B[0m w \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.3\u001B[39m \u001B[38;5;66;03m# bar width\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'param_grid' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "b5b5d4d1",
   "metadata": {},
   "source": "### Test the model and generate the predictions"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-13T20:50:24.833613Z",
     "start_time": "2025-10-13T20:50:05.676837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from helpers import load_csv_data, create_csv_submission\n",
    "from implementations import (\n",
    "    reg_logistic_regression,\n",
    "    reg_logistic_regression_adam,\n",
    "    least_squares,\n",
    "    ridge_regression,\n",
    ")\n",
    "# If sigmoid not imported already:\n",
    "def sigmoid(z): return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# ====== CONFIG: paths used in dataprocessing.ipynb ======\n",
    "DATA_DIR = \"/Users/neilabenlamri/PycharmProjects/project-1-girl_power/data\"\n",
    "IMPUTE_NPZ = f\"{DATA_DIR}/dataset/impute_vals.npz\"\n",
    "SCALER_NPZ = f\"{DATA_DIR}/dataset/scaler_zscore.npz\"\n",
    "PCA_NPZ    = f\"{DATA_DIR}/dataset/pca_model.npz\"\n",
    "\n",
    "XTEST_CSV  = f\"{DATA_DIR}/dataset/x_test.csv\"\n",
    "XTRAIN_PCA = f\"{DATA_DIR}/dataset/pca_train_projection.csv\"   # already used in run.ipynb\n",
    "YTRAIN_CSV = f\"{DATA_DIR}/dataset/y_train.csv\"\n",
    "\n",
    "# ====== 1) Utilities to load artifacts & apply same preprocessing to TEST ======\n",
    "\n",
    "def load_feature_names_from_imputer(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    # saved by `clean_then_impute(..., save_path)`: contains 'fillers' and 'feature_names'\n",
    "    feature_names = [str(x) for x in d[\"feature_names\"]]\n",
    "    fillers = d[\"fillers\"].astype(float)\n",
    "    return feature_names, fillers\n",
    "\n",
    "def load_headers(csv_path):\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "    return header  # full header (incl. Id)\n",
    "\n",
    "def load_matrix(csv_path):\n",
    "    # loads all numeric cols except header\n",
    "    return np.genfromtxt(csv_path, delimiter=\",\", skip_header=1)\n",
    "\n",
    "def extract_by_names(X_full, full_header, wanted_names):\n",
    "    # Map name -> index, then take columns in that exact order\n",
    "    idx_map = {name: i for i, name in enumerate(full_header)}\n",
    "    col_idx = [idx_map[name] for name in wanted_names]\n",
    "    return X_full[:, col_idx]\n",
    "\n",
    "def apply_imputation(X, fillers):\n",
    "    X = X.copy()\n",
    "    # assume NaN are present where needed; use learned fillers (mode/median) from TRAIN\n",
    "    for j in range(X.shape[1]):\n",
    "        m = np.isnan(X[:, j])\n",
    "        if np.any(m):\n",
    "            X[m, j] = fillers[j]\n",
    "    return X\n",
    "\n",
    "def load_scaler(npz_path):\n",
    "    d = np.load(npz_path, allow_pickle=True)\n",
    "    return {\"mean\": d[\"mean\"].astype(float), \"std\": d[\"std\"].astype(float)}\n",
    "\n",
    "def transform_standardizer(matrix, scaler):\n",
    "    mean, std = scaler[\"mean\"], scaler[\"std\"]\n",
    "    return (matrix - mean) / std\n",
    "\n",
    "def pca_transform_npz(standardized_data, pca_npz_path):\n",
    "    X = np.asarray(standardized_data, dtype=float)\n",
    "    d = np.load(pca_npz_path, allow_pickle=True)\n",
    "    components = d[\"components\"]    # (p x k)\n",
    "    n_features = int(d[\"n_features\"][0])\n",
    "    assert X.shape[1] == n_features, f\"Expected {n_features} features, got {X.shape[1]}\"\n",
    "    return X @ components\n",
    "\n",
    "# ====== 2) Build Z_test by EXACTLY mirroring train preprocessing ======\n",
    "\n",
    "def build_Z_test_from_raw():\n",
    "    # a) load full test (with Id col in col 0)\n",
    "    X_full = load_matrix(XTEST_CSV)           # shape (n_test, 1 + p_full)\n",
    "    ids_test = X_full[:, 0].astype(int)\n",
    "    X_full = X_full[:, 1:]                    # drop Id\n",
    "\n",
    "    # b) select the same feature subset + order as TRAIN (from imputer npz)\n",
    "    full_header = load_headers(XTEST_CSV)[1:] # drop 'Id' to align with X_full\n",
    "    feat_names, fillers = load_feature_names_from_imputer(IMPUTE_NPZ)\n",
    "    X_sel = extract_by_names(X_full, full_header, feat_names)\n",
    "\n",
    "    # c) impute with TRAIN fillers\n",
    "    X_imp = apply_imputation(X_sel, fillers)\n",
    "\n",
    "    # d) standardize with TRAIN scaler\n",
    "    scaler = load_scaler(SCALER_NPZ)\n",
    "    X_std = transform_standardizer(X_imp, scaler)\n",
    "\n",
    "    # e) PCA project with TRAIN PCA\n",
    "    Z_test = pca_transform_npz(X_std, PCA_NPZ)\n",
    "    return ids_test, Z_test\n",
    "\n",
    "# ====== 3) Refit BEST model on FULL TRAIN (your CV said ridge_regression best) ======\n",
    "\n",
    "def refit_best_on_full(best_method):\n",
    "    method = best_method[\"method\"]\n",
    "    lam    = best_method[\"lambda_\"]\n",
    "    gam    = best_method[\"gamma\"]\n",
    "    max_it = best_method[\"max_iter\"]\n",
    "\n",
    "    # We refit on the PCA train we already loaded/used in run.ipynb:\n",
    "    Z_train = np.loadtxt(XTRAIN_PCA, delimiter=\",\", dtype=float, skiprows=1)\n",
    "    y_train = np.loadtxt(YTRAIN_CSV, delimiter=\",\", dtype=float, skiprows=1, usecols=1)\n",
    "    # Our run.ipynb converts {-1,1} -> {0,1} for CV; we keep same target encoding if needed\n",
    "    # If we kept y in {0,1} during CV, keep it consistent here:\n",
    "    y_tr = np.where(y_train == -1, 0, y_train)  # match CV\n",
    "\n",
    "    if method == \"reg_logistic_regression\":\n",
    "        w_opt, _ = reg_logistic_regression(y_tr, Z_train, lam, np.zeros(Z_train.shape[1]), max_it, gam)\n",
    "    elif method == \"adam_reg_logistic_regression\":\n",
    "        w_opt, _ = reg_logistic_regression_adam(y_tr, Z_train, lam, np.zeros(Z_train.shape[1]), max_it, 0.9, 0.999, gam, 700)\n",
    "    elif method == \"least_squares\":\n",
    "        w_opt, _ = least_squares(y_tr, Z_train)\n",
    "    elif method == \"ridge_regression\":\n",
    "        w_opt, _ = ridge_regression(y_tr, Z_train, lam)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "    return w_opt, method\n",
    "\n",
    "# ====== 4) Predict on TEST and write AIcrowd CSV ======\n",
    "\n",
    "def predict_labels(X, w, method):\n",
    "    # Match your CV decision rules:\n",
    "    if method in [\"reg_logistic_regression\", \"adam_reg_logistic_regression\"]:\n",
    "        scores = sigmoid(X @ w)\n",
    "        y01 = (scores >= 0.5).astype(int)\n",
    "    else:  # least_squares, ridge_regression\n",
    "        scores = X @ w\n",
    "        y01 = (scores >= 0.5).astype(int)\n",
    "    # AIcrowd expects {-1, 1}\n",
    "    y = y01.copy()\n",
    "    y[y == 0] = -1\n",
    "    return y\n",
    "\n",
    "def make_submission(best_method, out_path=\"submission.csv\"):\n",
    "    # Build Z_test using the saved artifacts from dataprocessing.ipynb\n",
    "    test_ids, Z_test = build_Z_test_from_raw()\n",
    "\n",
    "    # Refit best model on FULL TRAIN (Z_train)\n",
    "    w_best, method = refit_best_on_full(best_method)\n",
    "\n",
    "    # Predict and write CSV\n",
    "    y_pred = predict_labels(Z_test, w_best, method)\n",
    "    create_csv_submission(test_ids, y_pred, out_path)\n",
    "    print(f\"[OK] Saved {out_path} with {len(y_pred)} rows — method={method}, λ={best_method['lambda_']}\")\n",
    "\n",
    "# === Run it ===\n",
    "make_submission(best_method, out_path=\"submission.csv\")\n"
   ],
   "id": "a6e3d5adfbc927fa",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/neilabenlamri/PycharmProjects/project-1-girl_power/data/dataset/scaler_zscore.npz'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 153\u001B[0m\n\u001B[1;32m    150\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m[OK] Saved \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mout_path\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(y_pred)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m rows — method=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmethod\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, λ=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbest_method[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlambda_\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m# === Run it ===\u001B[39;00m\n\u001B[0;32m--> 153\u001B[0m \u001B[43mmake_submission\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_method\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msubmission.csv\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[15], line 142\u001B[0m, in \u001B[0;36mmake_submission\u001B[0;34m(best_method, out_path)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mmake_submission\u001B[39m(best_method, out_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msubmission.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;66;03m# Build Z_test using the saved artifacts from dataprocessing.ipynb\u001B[39;00m\n\u001B[0;32m--> 142\u001B[0m     test_ids, Z_test \u001B[38;5;241m=\u001B[39m \u001B[43mbuild_Z_test_from_raw\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;66;03m# Refit best model on FULL TRAIN (Z_train)\u001B[39;00m\n\u001B[1;32m    145\u001B[0m     w_best, method \u001B[38;5;241m=\u001B[39m refit_best_on_full(best_method)\n",
      "Cell \u001B[0;32mIn[15], line 90\u001B[0m, in \u001B[0;36mbuild_Z_test_from_raw\u001B[0;34m()\u001B[0m\n\u001B[1;32m     87\u001B[0m X_imp \u001B[38;5;241m=\u001B[39m apply_imputation(X_sel, fillers)\n\u001B[1;32m     89\u001B[0m \u001B[38;5;66;03m# d) standardize with TRAIN scaler\u001B[39;00m\n\u001B[0;32m---> 90\u001B[0m scaler \u001B[38;5;241m=\u001B[39m \u001B[43mload_scaler\u001B[49m\u001B[43m(\u001B[49m\u001B[43mSCALER_NPZ\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     91\u001B[0m X_std \u001B[38;5;241m=\u001B[39m transform_standardizer(X_imp, scaler)\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# e) PCA project with TRAIN PCA\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[15], line 58\u001B[0m, in \u001B[0;36mload_scaler\u001B[0;34m(npz_path)\u001B[0m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload_scaler\u001B[39m(npz_path):\n\u001B[0;32m---> 58\u001B[0m     d \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnpz_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallow_pickle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m: d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmean\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstd\u001B[39m\u001B[38;5;124m\"\u001B[39m: d[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstd\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mfloat\u001B[39m)}\n",
      "File \u001B[0;32m~/anaconda3/envs/project1-grading/lib/python3.9/site-packages/numpy/lib/npyio.py:390\u001B[0m, in \u001B[0;36mload\u001B[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001B[0m\n\u001B[1;32m    388\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 390\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    391\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    393\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/neilabenlamri/PycharmProjects/project-1-girl_power/data/dataset/scaler_zscore.npz'"
     ]
    }
   ],
   "execution_count": 15
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
