{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fe5b554-b094-4c6f-975f-5142f082b7a8",
   "metadata": {},
   "source": [
    "**installing and importing useful libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a2f0f6f-c322-4fd3-b08e-5d934c84cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_theme()\n",
    "\n",
    "#importing optimization techniques\n",
    "from implementations import *\n",
    "from cross_validation import *\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa20b222-c0c6-4fdd-b500-885542b16cb3",
   "metadata": {},
   "source": [
    "**importing data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a9a5b36-c5a1-426f-a6eb-15117ba48dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (328135, 60)\n",
      "y_train shape: (328135,)\n"
     ]
    }
   ],
   "source": [
    "#IF WE USE OUTLIERS DETECTION WE SHOULD ACHANGE Y TRAIN\n",
    "\n",
    "\n",
    "# Paths to the CSV files ---- > change with yours\n",
    "datapath_1 = r\"C:\\Users\\sanni\\OneDrive\\Desktop\\POLIMI\\EPFL\\ML\\Project_1\\project-1-girl_power\\data\\pca_train_projection.csv\"\n",
    "datapath_2 = r\"C:\\Users\\sanni\\OneDrive\\Desktop\\POLIMI\\EPFL\\ML\\Project_1\\project-1-girl_power\\data\\dataset\\dataset\\y_train.csv\"\n",
    "\n",
    "# Load the data\n",
    "x_train = np.loadtxt(datapath_1, delimiter=',', dtype=float, skiprows=1)\n",
    "y_train = np.loadtxt(datapath_2, delimiter=',' , dtype=float, skiprows=1, usecols=1)\n",
    "\n",
    "# Convert y_train from {-1, 1} to {0, 1}\n",
    "y_train = np.where(y_train == -1, 0, y_train)\n",
    "\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88279ec",
   "metadata": {},
   "source": [
    "### Splitting the data between 20 % validation set and 80 % training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7488fbfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(262508, 60)\n",
      "(65627, 60)\n"
     ]
    }
   ],
   "source": [
    "def train_val_split(X, y, val_ratio=0.20, seed=42):\n",
    "    X = np.asarray(X); y = np.asarray(y)\n",
    "    n = X.shape[0]\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = np.arange(n)\n",
    "    rng.shuffle(idx)\n",
    "\n",
    "    n_val = int(np.round(n * val_ratio))\n",
    "    val_idx = idx[:n_val]\n",
    "    train_idx = idx[n_val:]\n",
    "\n",
    "    return X[train_idx], X[val_idx], y[train_idx], y[val_idx], train_idx, val_idx\n",
    "\n",
    "\n",
    "\n",
    "x_train_split, x_val_split, y_train_split, y_val_split, tr_idx, va_idx = train_val_split(x_train, y_train, val_ratio=0.20, seed=42)\n",
    "print(x_train_split.shape)\n",
    "print(x_val_split.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a26b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERSAMPLING AND BALANCING DATA\n",
    "def make_balanced_subset(x_train_filtered, y_train, majority_class=-1, minority_class=1,\n",
    "                         seed_major=0, seed_minor=42, seed_shuffle=7):\n",
    "    # Boolean masks\n",
    "    maj_mask = (y_train == majority_class)\n",
    "    min_mask = (y_train == minority_class)\n",
    "\n",
    "    # Indices per class\n",
    "    maj_idx = np.nonzero(maj_mask)[0]\n",
    "    min_idx = np.nonzero(min_mask)[0]\n",
    "\n",
    "    # Target size = size of minority (undersample majority)\n",
    "    n = len(min_idx)\n",
    "\n",
    "    # Sample without replacement\n",
    "    rs_maj = np.random.RandomState(seed_major)\n",
    "    rs_min = np.random.RandomState(seed_minor)\n",
    "    sampled_maj = rs_maj.choice(maj_idx, size=n, replace=False)\n",
    "    sampled_min = rs_min.choice(min_idx, size=n, replace=False)\n",
    "\n",
    "    # Combine and shuffle\n",
    "    balanced_idx = np.concatenate([sampled_maj, sampled_min])\n",
    "    rs_shuf = np.random.RandomState(seed_shuffle)\n",
    "    rs_shuf.shuffle(balanced_idx)\n",
    "\n",
    "    # Slice arrays\n",
    "    x_bal = x_train_filtered[balanced_idx]\n",
    "    y_bal = y_train[balanced_idx]\n",
    "    return x_bal, y_bal, balanced_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE DATA --- > not needed with pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea7578a-8712-481d-a6d8-9a12ba3e71dd",
   "metadata": {},
   "source": [
    "### Hyperparameters definition and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cf219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lambdas = [ 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1]  # regularization parameters list\n",
    "#gammas = [1e-4, 1e-3, 1e-2, 1e-1, 1] # step-size parameters list\n",
    "#max_iters = [100, 1000, 10000] # max iters list\n",
    "\n",
    "lambdas = [ 1e-4, 1e-3, 1e-2, 1e-1]  # regularization parameters list\n",
    "gammas = [1e-4, 1e-3] # step-size parameters list\n",
    "max_iters = [100] # max iters list\n",
    "\n",
    "\n",
    "def compute_auc(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    AUC calculation using Mann-Whitney statistics\n",
    "    Inputs : \n",
    "            - y_true : numpy array containing the real {0, 1} values of the dataset\n",
    "            - y_scores : numpy array containing our predictions\n",
    "    Output : \n",
    "            AUC Area under the ROC curve \n",
    "    \"\"\"\n",
    "    order = np.argsort(y_scores)\n",
    "    y_true_sorted = y_true[order]\n",
    "\n",
    "    n_pos = np.sum(y_true)\n",
    "    n_neg = len(y_true) - n_pos\n",
    "\n",
    "    # rank positions \n",
    "    rank_positions = np.arange(1, len(y_true_sorted) + 1)\n",
    "    rank_sum = np.sum(rank_positions[y_true_sorted == 1])\n",
    "\n",
    "    # AUC using Mann–Whitney\n",
    "    auc = (rank_sum - n_pos*(n_pos+1)/2) / (n_pos * n_neg)\n",
    "    return auc\n",
    "\n",
    "def compute_accuracy(y_true, y_scores) : \n",
    "    \"\"\"\n",
    "    Accuracy computation\n",
    "    Inputs : \n",
    "            - y_true : numpy array containing the real {0, 1} values of the dataset\n",
    "            - y_scores : numpy array containing our predictions\n",
    "    Output : \n",
    "            Accuracy = correct predictions / total predictions %\n",
    "\n",
    "    \"\"\"\n",
    "    correct_pred = (y_true == y_scores)\n",
    "    accuracy = np.mean(correct_pred) * 100\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa264a6",
   "metadata": {},
   "source": [
    "### K-fold cross validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9431e08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For method reg_logistic_regression, best λ=0.0001, γ=0.001, max_iter=100\n",
      "For method least_squares, best λ=0.0001, γ=0.0001, max_iter=100\n",
      "For method adam_reg_logistic_regression, best λ=0.01, γ=0.001, max_iter=100\n",
      "For method ridge_regression, best λ=0.01, γ=0.0001, max_iter=100\n",
      "The best method is ridge_regression, with best λ=0.01, γ=0.0001, max_iter=100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_k_indices(N, k_fold, seed=21): \n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        N:      num of samples\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    \"\"\"\n",
    "    num_row = N\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "     \n",
    "\n",
    "def k_fold_cross_validation(y_train, x_train, lambdas, gammas, max_iters, k_fold, methods, seed, oversampling = True) :\n",
    "    # dictionary to contain the best method with the best parameters and its metrics\n",
    "    best_overall = {\"method\": \"\", \"lambda_\": 0, \"gamma\": 0, \"max_iter\": 0, \"train loss\": 0, \"test loss\": 0, \"AUC\": 0, \"accuracy\": 0, \"y_pred\" : None, \"w_opt\": None}\n",
    "    results =[] # to keep the best results per method\n",
    "    #creating k folders on the train set \n",
    "    k_indices = build_k_indices(len(y_train), k_fold, seed)\n",
    "\n",
    "    for method in methods : #scrolling methods\n",
    "        best_per_method =  {\"method\": \"\", \"lambda_\": 0, \"gamma\": 0, \"max_iter\": 0, \"train loss\": 0, \"test loss\": 0, \"AUC\": 0, \"accuracy\": 0, \"y_pred\" : None, \"w_opt\": None} \n",
    "        for lam in lambdas : # scrolling lambdas\n",
    "            for gam in gammas : #scrolling gammas\n",
    "                for max_it in max_iters : #scrolling iters ----> model defined at this point\n",
    "                    logistic_loss_tr = []\n",
    "                    logistic_loss_te =[]\n",
    "                    AUC=  []\n",
    "                    accuracies = []\n",
    "                    \n",
    "                    for k in range(k_fold) : \n",
    "                        #k-th subgroup in test, others in train\n",
    "                        test_mask = np.isin(np.arange(len(y_train)), k_indices[k, :])\n",
    "                        y_test_k = y_train[test_mask]\n",
    "                        x_test_k = x_train[test_mask]\n",
    "    \n",
    "                        y_train_k=y_train[~test_mask]\n",
    "                        x_train_k=x_train[~test_mask]\n",
    "                        \n",
    "                        if oversampling :\n",
    "                            #oversampling dataset\n",
    "                            x_train_k , y_train_k, _ = make_balanced_subset(x_train_k, y_train_k, 0, 1, seed_major=0, seed_minor=42, seed_shuffle=7)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                        #train the model\n",
    "                        if method == \"reg_logistic_regression\" :\n",
    "                            w_opt, loss = reg_logistic_regression(y_train_k, x_train_k,lam, np.zeros(x_train_k.shape[1]), max_it, gam)\n",
    "                        elif method == \"least_squares\" :\n",
    "                            w_opt, loss = least_squares(y_train_k, x_train_k)\n",
    "                        elif method == \"adam_reg_logistic_regression\":\n",
    "                            w_opt, loss = reg_logistic_regression_adam(y_train_k, x_train_k, lam, np.zeros(x_train_k.shape[1]), max_it, 0.9, 0.999, gam, 700 )\n",
    "                        elif method == \"ridge_regression\" :\n",
    "                            w_opt, loss = ridge_regression(y_train_k, x_train_k, lam)\n",
    "\n",
    "                        #computing metrics \n",
    "                        logistic_loss_tr.append(compute_logistic_loss(y_train_k, x_train_k, w_opt)) #without penalizing term\n",
    "                        logistic_loss_te.append(compute_logistic_loss(y_test_k, x_test_k, w_opt)) #without penalizing term CAPIRE CHE SENSO HA COMPARARE STE LOSS\n",
    "\n",
    "                        if method in [\"adam_reg_logistic_regression\", \"reg_logistic_regression\"]:\n",
    "                            pred = sigmoid(x_test_k @ w_opt) \n",
    "                        else:\n",
    "                            pred = x_test_k @ w_opt\n",
    "\n",
    "                        AUC.append(compute_auc(y_test_k, pred))\n",
    "                        accuracies.append(compute_accuracy(y_test_k, (pred >= 0.5).astype(int)))\n",
    "                    # updating    \n",
    "                    if np.mean(AUC) > best_per_method[\"AUC\"]:\n",
    "                        best_per_method.update({\"method\": method, \"gamma\": gam, \"lambda_\": lam, \"max_iter\": max_it, \"train loss\": np.mean(logistic_loss_tr), \"test loss\": np.mean(logistic_loss_te), \"AUC\": np.mean(AUC), \"accuracy\": np.mean(accuracies), \"y_pred\" : (pred >= 0.5).astype(int) , \"w_opt\": w_opt})\n",
    "\n",
    "                    if np.mean(AUC) > best_overall[\"AUC\"]:\n",
    "                        best_overall.update({\"method\": method, \"gamma\": gam, \"lambda_\": lam, \"max_iter\": max_it, \"train loss\": np.mean(logistic_loss_tr), \"test loss\": np.mean(logistic_loss_te), \"AUC\": np.mean(AUC), \"accuracy\": np.mean(accuracies), \"y_pred\" : (pred >= 0.5).astype(int) , \"w_opt\": w_opt})\n",
    "\n",
    "        results.append(best_per_method)\n",
    "        print(f\"For method {method}, best λ={best_per_method['lambda_']}, γ={best_per_method['gamma']}, max_iter={best_per_method['max_iter']}\")\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"The best method is {best_overall['method']}, with best λ={best_overall['lambda_']}, γ={best_overall['gamma']}, max_iter={best_overall['max_iter']}\")\n",
    "\n",
    "\n",
    "\n",
    "    return best_overall, results \n",
    "\n",
    "        \n",
    "methods=[\"reg_logistic_regression\", \"least_squares\", \"adam_reg_logistic_regression\", \"ridge_regression\"]\n",
    "best_method, results = k_fold_cross_validation(y_train_split, x_train_split, lambdas, gammas, max_iters, 5, methods, seed = 21, oversampling=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e3ac5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'ridge_regression', 'lambda_': 0.01, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6817317752542131, 'test loss': 0.6817415970302714, 'AUC': 0.8489835054683796, 'accuracy': 91.2089293537266, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.50058611e-02, -8.87536146e-03,  1.65235944e-03, -5.91319982e-04,\n",
      "        4.64029179e-03,  1.18368460e-02, -1.03447114e-02, -1.21375923e-02,\n",
      "       -2.11234177e-02, -3.95062146e-04,  1.01048533e-02, -6.49793695e-03,\n",
      "        2.72864303e-03,  2.01904766e-02, -5.96512952e-03, -1.69934947e-02,\n",
      "       -6.66632778e-03,  7.57231662e-03,  7.50320969e-03,  7.34997888e-03,\n",
      "        1.57390387e-02, -9.80302288e-04, -5.27560938e-04, -5.84386092e-03,\n",
      "        4.65541021e-03, -9.00916660e-03,  6.43199674e-03, -1.06177109e-02,\n",
      "       -6.08629998e-03,  4.28472084e-03, -2.26993426e-03, -8.33775882e-04,\n",
      "        6.86705614e-03,  2.30045709e-03, -1.38317554e-03, -5.93250408e-03,\n",
      "        5.17540744e-03,  1.39433428e-03, -1.26820932e-03,  1.08241581e-02,\n",
      "        6.31206313e-03, -2.48289074e-04,  9.61153621e-05,  3.59771866e-03,\n",
      "        1.80613280e-03, -1.31552199e-03,  2.59202691e-04, -1.15472414e-03,\n",
      "        1.83046233e-03,  1.17768176e-03,  3.37029318e-03,  1.48083364e-03,\n",
      "       -2.31938605e-03, -1.36003159e-04, -2.92437596e-04, -5.59604211e-03,\n",
      "       -3.08707316e-03, -3.64404942e-03, -5.39279905e-03, -1.50327400e-03])}\n",
      "[{'method': 'reg_logistic_regression', 'lambda_': 0.0001, 'gamma': 0.001, 'max_iter': 100, 'train loss': 0.6868764418273539, 'test loss': 0.6868852975468813, 'AUC': 0.8343951963000361, 'accuracy': 61.66815870173902, 'y_pred': array([0, 1, 1, ..., 0, 0, 0]), 'w_opt': array([ 2.39176791e-02, -4.18510712e-03,  4.64537601e-04, -1.86198803e-04,\n",
      "        1.19102402e-03,  2.12855812e-03, -1.69820645e-03, -1.86734077e-03,\n",
      "       -3.03861541e-03, -4.58990592e-05,  1.39984307e-03, -9.50850183e-04,\n",
      "        4.08719594e-04,  2.23679671e-03, -5.74858732e-04, -1.84581964e-03,\n",
      "       -7.57491968e-04,  8.98689096e-04,  7.33681419e-04,  6.82368385e-04,\n",
      "        1.57912399e-03,  4.87251314e-05, -8.99664312e-05, -4.74745176e-04,\n",
      "        4.55995950e-04, -8.22153171e-04,  6.14966521e-04, -9.74637419e-04,\n",
      "       -5.82138664e-04,  3.68051482e-04, -2.06571775e-04, -6.08454347e-06,\n",
      "        4.67779161e-04,  1.54233799e-04, -2.46036301e-05, -4.44932536e-04,\n",
      "        4.82357000e-04,  1.07745799e-04, -1.81061364e-04,  7.76454933e-04,\n",
      "        3.99875576e-04,  2.77720971e-05,  6.81512065e-05,  3.16044042e-04,\n",
      "        1.20335115e-04, -1.19293123e-04,  2.33887717e-05, -5.84245976e-05,\n",
      "        2.02195924e-04,  3.42810105e-05,  2.21595787e-04,  1.66021235e-04,\n",
      "       -1.52026415e-04, -4.82611150e-06,  2.15257462e-05, -3.85422032e-04,\n",
      "       -1.65501998e-04, -1.78700388e-04, -2.79104780e-04, -1.24887253e-04])}, {'method': 'least_squares', 'lambda_': 0.0001, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6816663068069608, 'test loss': 0.6816764041256543, 'AUC': 0.8488572131059865, 'accuracy': 91.21654825622369, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.51013108e-02, -8.91045099e-03,  1.66287619e-03, -5.96242010e-04,\n",
      "        4.68221158e-03,  1.19616530e-02, -1.04608280e-02, -1.22907316e-02,\n",
      "       -2.14178750e-02, -4.00719735e-04,  1.02560556e-02, -6.59671068e-03,\n",
      "        2.77323092e-03,  2.05460899e-02, -6.07286979e-03, -1.73069126e-02,\n",
      "       -6.79517362e-03,  7.72016919e-03,  7.65134931e-03,  7.49719025e-03,\n",
      "        1.60669849e-02, -9.99682473e-04, -5.37790322e-04, -5.97064696e-03,\n",
      "        4.75709824e-03, -9.21071031e-03,  6.57420809e-03, -1.08583286e-02,\n",
      "       -6.22567688e-03,  4.38273252e-03, -2.32268638e-03, -8.54338276e-04,\n",
      "        7.03347460e-03,  2.36174752e-03, -1.41912387e-03, -6.08663644e-03,\n",
      "        5.30588144e-03,  1.43109757e-03, -1.30313061e-03,  1.11316413e-02,\n",
      "        6.49138945e-03, -2.55929690e-04,  9.75918564e-05,  3.70688147e-03,\n",
      "        1.86295839e-03, -1.35300257e-03,  2.67050424e-04, -1.19017218e-03,\n",
      "        1.89169814e-03,  1.21711690e-03,  3.47813045e-03,  1.53363102e-03,\n",
      "       -2.40057573e-03, -1.40105606e-04, -3.00626865e-04, -5.80349537e-03,\n",
      "       -3.20450056e-03, -3.78637093e-03, -5.60142140e-03, -1.55824967e-03])}, {'method': 'adam_reg_logistic_regression', 'lambda_': 0.01, 'gamma': 0.001, 'max_iter': 100, 'train loss': 0.67040576918394, 'test loss': 0.670601758308936, 'AUC': 0.840694588921808, 'accuracy': 65.73055751319023, 'y_pred': array([0, 1, 0, ..., 0, 0, 0]), 'w_opt': array([ 8.59692338e-02, -3.71127243e-02, -8.28468401e-05,  7.06085581e-04,\n",
      "        2.69871478e-02,  3.77067302e-02, -3.40772167e-02, -4.07044328e-02,\n",
      "       -6.13954034e-02,  3.29182791e-03,  3.57008398e-02, -2.28271226e-02,\n",
      "        1.61209330e-02,  4.57077999e-02, -1.94274081e-02, -4.42810871e-02,\n",
      "       -2.19824017e-02,  2.33421779e-02,  2.52441559e-02,  2.07310918e-02,\n",
      "        4.77800801e-02, -2.32115187e-03, -6.88687570e-04, -8.54127425e-03,\n",
      "        1.61057537e-02, -2.94110377e-02,  2.75864917e-02, -2.94041534e-02,\n",
      "       -1.22806750e-02, -5.34647730e-03, -1.67659667e-02, -5.36190575e-03,\n",
      "        1.89660168e-02,  1.77318052e-02,  5.11344295e-03, -1.97821075e-02,\n",
      "        4.07013873e-02,  1.08084824e-02, -2.11204120e-03,  3.40293735e-02,\n",
      "        1.91910424e-02,  9.88971435e-03,  1.08617026e-02,  2.28192809e-02,\n",
      "        1.97252273e-03,  5.04280096e-03,  3.29842535e-03,  8.88114368e-03,\n",
      "        1.34757999e-02, -1.52003051e-02,  1.65159835e-02,  9.36965306e-03,\n",
      "        7.17799210e-03,  3.80876095e-03, -1.42745788e-03, -1.47771845e-02,\n",
      "       -5.54851979e-03, -5.96231442e-03, -9.49934491e-03, -1.46743800e-03])}, {'method': 'ridge_regression', 'lambda_': 0.01, 'gamma': 0.0001, 'max_iter': 100, 'train loss': 0.6817317752542131, 'test loss': 0.6817415970302714, 'AUC': 0.8489835054683796, 'accuracy': 91.2089293537266, 'y_pred': array([0, 0, 0, ..., 0, 0, 0]), 'w_opt': array([ 3.50058611e-02, -8.87536146e-03,  1.65235944e-03, -5.91319982e-04,\n",
      "        4.64029179e-03,  1.18368460e-02, -1.03447114e-02, -1.21375923e-02,\n",
      "       -2.11234177e-02, -3.95062146e-04,  1.01048533e-02, -6.49793695e-03,\n",
      "        2.72864303e-03,  2.01904766e-02, -5.96512952e-03, -1.69934947e-02,\n",
      "       -6.66632778e-03,  7.57231662e-03,  7.50320969e-03,  7.34997888e-03,\n",
      "        1.57390387e-02, -9.80302288e-04, -5.27560938e-04, -5.84386092e-03,\n",
      "        4.65541021e-03, -9.00916660e-03,  6.43199674e-03, -1.06177109e-02,\n",
      "       -6.08629998e-03,  4.28472084e-03, -2.26993426e-03, -8.33775882e-04,\n",
      "        6.86705614e-03,  2.30045709e-03, -1.38317554e-03, -5.93250408e-03,\n",
      "        5.17540744e-03,  1.39433428e-03, -1.26820932e-03,  1.08241581e-02,\n",
      "        6.31206313e-03, -2.48289074e-04,  9.61153621e-05,  3.59771866e-03,\n",
      "        1.80613280e-03, -1.31552199e-03,  2.59202691e-04, -1.15472414e-03,\n",
      "        1.83046233e-03,  1.17768176e-03,  3.37029318e-03,  1.48083364e-03,\n",
      "       -2.31938605e-03, -1.36003159e-04, -2.92437596e-04, -5.59604211e-03,\n",
      "       -3.08707316e-03, -3.64404942e-03, -5.39279905e-03, -1.50327400e-03])}]\n"
     ]
    }
   ],
   "source": [
    "print(best_method) \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59e6c59",
   "metadata": {},
   "source": [
    "### Validation on our validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81de6620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best method ridge_regression has an accuracy = 91.3069315982751 and an AUC = 0.8464946861089571 on our validation set\n"
     ]
    }
   ],
   "source": [
    "def validation(w_opt, x_val, y_val): \n",
    "    w_opt = best_method[\"w_opt\"]\n",
    "    if best_method[\"method\"] in [\"reg_logistic_regression\", \"adam_reg_logistic_regression\"] :\n",
    "        predictions = sigmoid(x_val @ w_opt) \n",
    "    else:\n",
    "        predictions = x_val @ w_opt\n",
    "    AUC = compute_auc(y_val, predictions)\n",
    "    accuracy = compute_accuracy(y_val, (predictions>=0.5).astype(int))\n",
    "\n",
    "    return AUC, accuracy\n",
    "\n",
    "\n",
    "AUC, accuracy = validation(best_method[\"w_opt\"], x_val_split, y_val_split) \n",
    "\n",
    "print(f\"the best method {best_method['method']} has an accuracy = {accuracy} and an AUC = {AUC} on our validation set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab264da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC calcolata manualmente: 0.64\n",
      "AUC con scikit-learn: 0.64\n",
      "Differenza: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cross_validation_visualization(param_grid, logistic_loss_tr, logistic_loss_te)\n",
    "\n",
    "num_par = len(param_grid)\n",
    "w = 0.3 # bar width\n",
    "pos = np.arange(num_par)\n",
    "plt.bar(pos - w, AUC, width = w, label='AUC' )\n",
    "plt.bar(pos, accuracies, width=w, label= 'Accuracy')\n",
    "plt.bar(pos + w, logistic_loss_tr, width = w, label = 'train logistic loss' )\n",
    "plt.bar(pos + w, logistic_loss_te, width=w, label= \"test logistic loss\" )\n",
    "\n",
    "plt.xticks(pos, param_grid)\n",
    "plt.xlabel('Different regularization hyperparameter values')\n",
    "plt.title('Finding the best regularization hyperparamter - ADAM case')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b5d4d1",
   "metadata": {},
   "source": [
    "Now that we have found the best regularization hyperparameter for Adam reg log and GD reg log, let's test which model is the best to make predictions using a k-fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CONFRONTING ALL THE MODELS WITH K-FOLD CROSS VALIDATION \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "N = len(y_train)\n",
    "d = x_train.shape[1]\n",
    "k = 10  # folds\n",
    "initial_w = np.zeros(d, )\n",
    "max_iters = 10000\n",
    "gamma = 0.01\n",
    "beta_1 = 0.9 #using Adam paper as benchmark\n",
    "beta_2 = 0.999 #using Adam paper as benchmark\n",
    "mini_batch_size = 700 #using Adam paper as benchmark \n",
    "\n",
    "models = [\n",
    "    (\"MSE GD\", lambda: mean_squared_error_gd(initial_w=initial_w, max_iters=max_iters, gamma=gamma)),\n",
    "    (\"MSE SGD\", lambda: mean_squared_error_sgd(initial_w=initial_w, max_iters=max_iters, gamma=gamma, mini_batch_size=mini_batch_size)),\n",
    "    (\"Least Squares\", lambda: least_squares()),\n",
    "    (\"Ridge Regression\", lambda: ridge_regression()), #understand which lambda here \n",
    "    (\"Logistic Regression GD\", lambda: logistic_regression(initial_w=initial_w, max_iters=max_iters, gamma=gamma)),\n",
    "    (\"Reg Logistic ADAM\", lambda: reg_logistic_regression_adam(lambda_adam, initial_w=initial_w, max_iters=max_iters,  beta_1=beta_1, beta_2=beta_2,gamma=gamma, mini_batch_size=mini_batch_size)),\n",
    "    (\"Reg Logistic GD\", lambda: reg_logistic_regression(lambda_gd, initial_w=initial_w, max_iters=max_iters, gamma=gamma))\n",
    "]  ### does it make sense to include all the models ????\n",
    "\n",
    "\n",
    "#### validation metrics\n",
    "logistic_loss_tr = []\n",
    "logistic_loss_te =[]\n",
    "AUC=  []\n",
    "accuracies = []\n",
    "\n",
    "####  k-fold cross validation\n",
    "k_indices =build_k_indices(N, k_fold, seed)\n",
    "for model in models:\n",
    "    fold_loss_tr = 0\n",
    "    fold_loss_te = 0\n",
    "    fold_AUC = 0\n",
    "    fold_accuracy = 0\n",
    "\n",
    "    for k in range(k_fold):\n",
    "        #k-th subgroup in test, others in train\n",
    "        test_mask = np.isin(np.arange(len(y)), k_indices[k, :])\n",
    "        y_test_k = y_train[test_mask]\n",
    "        x_test_k = x_train[test_mask]\n",
    "    \n",
    "        y_train_k=y_train[~test_mask]\n",
    "        x_train_k=x_train[~test_mask]\n",
    "    \n",
    "        # train the model\n",
    "        name, model_fn = model\n",
    "        w_opt, loss_tr = model_fn(y_train_k, x_train_k)\n",
    "    \n",
    "        # calculate the loss for test data\n",
    "        loss_te = compute_logistic_loss(y_test_k, x_test_k, w_opt)\n",
    "\n",
    "        # compute AUC \n",
    "        predictions = sigmoid (x_test_k @ w_opt)\n",
    "        AUC = compute_auc(y_test_k, predictions)\n",
    "\n",
    "        # compute accuracy\n",
    "        y_pred = (predictions >= 0.5).astype(int)\n",
    "        accuracy = np.mean(y_pred == y_test)\n",
    "       \n",
    "\n",
    "        # storing validation results\n",
    "        fold_loss_tr += loss_tr\n",
    "        fold_loss_te+=loss_te\n",
    "        fold_AUC += AUC\n",
    "        fold_accuracy += accuracy\n",
    "    logistic_loss_tr.append(fold_loss_tr/k_fold)\n",
    "    logistic_loss_te.append(fold_loss_te/k_fold)\n",
    "    AUC.append(fold_AUC/k_fold)\n",
    "    accuracies.append(fold_accuracy/k_fold)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Plotting results per model   \n",
    "\n",
    "num_models = len(models)\n",
    "w = 0.3\n",
    "pos = np.arange(num_models)\n",
    "labels = [name for name, _ in models]\n",
    "\n",
    "plt.bar(pos - w, AUC, width=w, label='AUC')\n",
    "plt.bar(pos, accuracies, width=w, label='Accuracy')\n",
    "plt.bar(pos + w, logistic_loss_te, width=w, label='test logistic loss')\n",
    "plt.bar(pos + 2*w, logistic_loss_tr, width=w, label='train logistic loss')\n",
    "\n",
    "plt.xticks(pos, labels, rotation=45)\n",
    "plt.xlabel('Different models')\n",
    "plt.title('Finding the best model')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
