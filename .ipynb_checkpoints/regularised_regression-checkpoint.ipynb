{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb6682",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "import datetime\n",
    "\n",
    "# Numerical computing\n",
    "import numpy as np\n",
    "\n",
    "#importing optimization functions\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b02edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_loss(y, tx, w)\n",
    "\"\"\"\n",
    "        Calculate logistic loss when y is in {0, 1}\n",
    "Args: \n",
    "        - y = numpy array of shape (N, ) containing training outputs\n",
    "        - tx = numpy array of shape (N, d) containing training inputs\n",
    "        - w = numpy array of shape (d, ) containing parameters\n",
    "Returns: \n",
    "        - loss = logistic loss value at w\n",
    "\"\"\"\n",
    "    #sample size\n",
    "    N = len(y)\n",
    "\n",
    "    #compute loss\n",
    "    z = tw @ w\n",
    "    loss = (1/N)*(np.sum(- y * z + np.log( 1 + np.exp(z))))\n",
    "\n",
    "return loss\n",
    "\n",
    "def compute_logistic_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w in logistic loss function case\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(d, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (d, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    #sample size\n",
    "    N=len(y)\n",
    "\n",
    "    #compute logistic function\n",
    "    z = tx @ w\n",
    "    sigma = sigmoid(z)\n",
    "    \n",
    "    #compute gradient\n",
    "    grad=(1/N)* tx.T @ (sigma - y)\n",
    "\n",
    "    return grad\n",
    "\n",
    "def logistic_gradient_descent(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,d)\n",
    "        initial_w: shape=(d, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "\n",
    "    Returns:\n",
    "        w: w optimal through GD\n",
    "        loss: loss function value evaluated at w optimal\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # computing gradient and loss\n",
    "        grad=compute_logistic_gradient(y, tx, w)\n",
    "        loss=compute_logistic_loss(y, tx, w)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    \"\"\"\n",
    "    Logistic regression using gradient descent (y ∈ {0, 1})\n",
    "\n",
    "    INPUTS:\n",
    "                - y = numpy  array of shape (N,) containing train outputs (0, 1)\n",
    "                - tx = numpy array of shape (N, d) containing train inputs\n",
    "                - initial_w = initial weight vector of paramters\n",
    "                - max_iters= max number of iterations allowed in gradient descendent algorithm\n",
    "                - gamma = step-size\n",
    "    OUTPUTS:\n",
    "                - w = numpy arraing containing the solution paramters\n",
    "                - loss= the loss function value corresponding to the solution parameters\n",
    "    \"\"\"\n",
    "\n",
    "    w, loss = logistic_gradient_descent(y, tx, initial_w, max_iters, gamma) #w optimal through logistic gradient descendent\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_reg_logistic_loss(y, tx, w,lambda_)\n",
    "\"\"\"\n",
    "        Calculate logistic loss when y is in {0, 1}\n",
    "Args: \n",
    "        - y = numpy array of shape (N, ) containing training outputs\n",
    "        - tx = numpy array of shape (N, d) containing training inputs\n",
    "        - w = numpy array of shape (d, ) containing parameters\n",
    "Returns: \n",
    "        - loss = logistic loss value at w\n",
    "\"\"\"\n",
    "    #sample size\n",
    "    N = len(y)\n",
    "\n",
    "    #compute loss\n",
    "    z = tw @ w\n",
    "    loss = (1/N)*(np.sum(- y * z + np.log( 1 + np.exp(z)) + lambda_*np.square(w)))\n",
    "\n",
    "return loss\n",
    "\n",
    "def compute_reg_logistic_gradient(y, tx, w, lambda_):\n",
    "    \"\"\"Computes the gradient at w in logistic loss function case\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,2)\n",
    "        w: shape=(d, ). The vector of model parameters.\n",
    "\n",
    "    Returns:\n",
    "        An array of shape (d, ) (same shape as w), containing the gradient of the loss at w.\n",
    "    \"\"\"\n",
    "    #sample size\n",
    "    N=len(y)\n",
    "\n",
    "    #compute logistic function\n",
    "    z = tx @ w\n",
    "    sigma = sigmoid(z)\n",
    "    \n",
    "    #compute gradient\n",
    "    grad=(1/N)* tx.T @ (sigma - y) + 2*lambda_*w\n",
    "\n",
    "    return grad\n",
    "\n",
    "def reg_logistic_gradient_descent(y,tx,lambda_,initial_w,max_iters,gamma):\n",
    "    \"\"\"The Regularised Logistic Gradient Descent (GD) algorithm.\n",
    "\n",
    "    Args:\n",
    "        y: shape=(N, )\n",
    "        tx: shape=(N,d)\n",
    "        initial_w: shape=(d, ). The initial guess (or the initialization) for the model parameters\n",
    "        max_iters: a scalar denoting the total number of iterations of GD\n",
    "        gamma: a scalar denoting the stepsize\n",
    "        lambda: regularisation term\n",
    "\n",
    "    Returns:\n",
    "        w: w optimal through GD\n",
    "        loss: loss function value evaluated at w optimal\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    w = initial_w\n",
    "    \n",
    "    for n_iter in range(max_iters):\n",
    "        # computing gradient and loss\n",
    "        grad=compute_reg_logistic_gradient(y, tx, w,lambda_)\n",
    "        loss=compute_reg_logistic_loss(y, tx, w, lambda_)\n",
    "        \n",
    "        # update w by gradient\n",
    "        w = w - gamma * grad\n",
    "\n",
    "    return w, loss\n",
    "\n",
    "def reg_logistic_regression(y,tx,lambda_,initial_w,max_iters,gamma):\n",
    "    \"\"\"\n",
    "    Regularised Logistic regression using gradient descent (y ∈ {0, 1})\n",
    "\n",
    "    INPUTS:\n",
    "                - y = numpy  array of shape (N,) containing train outputs (0, 1)\n",
    "                - tx = numpy array of shape (N, d) containing train inputs\n",
    "                - initial_w = initial weight vector of paramters\n",
    "                - max_iters= max number of iterations allowed in gradient descendent algorithm\n",
    "                - gamma = step-size\n",
    "                - lambda: regularisation term\n",
    "    OUTPUTS:\n",
    "                - w = numpy arraing containing the solution paramters\n",
    "                - loss= the loss function value corresponding to the solution parameters\n",
    "    \"\"\"\n",
    "    w, loss = reg_logistic_gradient_descent(y, tx, lambda_,initial_w, max_iters, gamma) #w optimal through logistic gradient descendent\n",
    "        \n",
    "        \n",
    "    return w, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
